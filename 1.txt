Exp3
bin/flume-ng agent -c ~/flume/ -f ~/flume/net.conf --name NetcatAgent -Dflume.root.logger=INFO,console
telnet locatelnet localhost 56565
EXP4
create database jd;
use jd;
create table jdemp(eid int , salary int);
insert into jdemp values(1,1),(2,2),(3,3);
select * from jdemp;
sqoop import --connect jdbc:mysql://localhost:3306/jd --table jdemp --username hadoop --password 123456 --target-dir /sqooptransfer/jd -m 1
create external table hiral1(eid int , sal int) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' LINES TERMINATED BY '\n' LOCATION '/sqooptransfer/jdemp';
select * from hiral1;
select * from hiral;
EXp5
hive
create database sak;

show databases;
create table employee(eid int,name string,salary int,destination string)
    > comment'Employee Details'
    > row format delimited
    > fields terminated by ','
    > lines terminated by '\n'
    > STORED AS TEXTFILE;
LOAD DATA LOCAL INPATH'/home/hadoop/Desktop/text.txt'OVERWRITE INTO TABLE employee;
select * from employee;
SELECT * FROM employee WHERE salary>30000;
SELECT eid,name,salary,destination FROM employee ORDER BY destination;
SELECT destination,count(*) FROM employee GROUP BY destination;

Exp6
use mongodemo


> db.inventory.insertMany([
...	{ item: "journal", qty: 25, size: { h: 14, w: 21, uom: "cm" }, status: "A" },
...	{ item: "notebook", qty: 50, size: { h: 8.5, w: 11, uom: "in" }, status: "A" },
...	{ item: "paper", qty: 100, size: { h: 8.5, w: 11, uom: "in" }, status: "D" },
...	{ item: "planner", qty: 75, size: { h: 22.85, w: 30, uom: "cm" }, status: "D" },
...	{ item: "postcard", qty: 45, size: { h: 10, w: 15.25, uom: "cm" }, status: "A" }
... ]);

> db.inventory.find( { status: "D" } )
{ "_id" : ObjectId("5db1724f4f048124990ab081"), "item" : "paper", "qty" : 100, "size" : { "h" : 8.5, "w" : 11, "uom" : "in" }, "status" : "D" }
{ "_id" : ObjectId("5db1724f4f048124990ab082"), "item" : "planner", "qty" : 75, "size" : { "h" : 22.85, "w" : 30, "uom" : "cm" }, "status" : "D" }
>



EXP 7
sqoop import --connect jdbc:mysql://localhost:3306/jd --table jdemp --username hadoop --password 123456 --target-dir /sqooptransfer/jd -m 1

pig -x local

grunt> sqoopdata = LOAD 'hdfs://localhost:8020/sqooptransfer/jd'USING PigStorage(',') AS (eid:int,sal:int);

grunt> output1 = FILTER sqoopdata BY sal > 0;
grunt> dump output1;


EXP9
from __future__ import print_function

import os
os.environ["HADOOP_HOME"] = "/home/hadoop/hadoop2"
os.environ["SPARK_HOME"] = "/home/hadoop/spark-2.3.1-bin-hadoop2.7"

import findspark
findspark.init()

import re
import sys
from operator import add

from pyspark.sql import SparkSession

def computeContribs(urls, rank):
    
    num_urls = len(urls)
    for url in urls:
        yield (url, rank / num_urls)


def parseNeighbors(urls):
    
    parts = re.split(r'\s+', urls)
    return parts[0], parts[1]


if __name__ == "__main__":
    if len(sys.argv) != 3:
        print("Usage: pagerank  ", file=sys.stderr)
        sys.exit(-1)

       spark = SparkSession\
        .builder\
        .appName("PythonPageRank")\
        .getOrCreate()

       lines = spark.read.text(sys.argv[1]).rdd.map(lambda r: r[0])

       links = lines.map(lambda urls: parseNeighbors(urls)).distinct().groupByKey().cache()
       print(links.count())
    n = links.count()


       ranks = links.map(lambda url_neighbors: (url_neighbors[0], 1))
  
    for iteration in range(int(sys.argv[2])):
        
        contribs = links.join(ranks).flatMap(
            lambda url_urls_rank: computeContribs(url_urls_rank[1][0], url_urls_rank[1][1]))

        
        ranks = contribs.reduceByKey(add).mapValues(lambda rank: rank * 0.85 + 0.15)

    
    for (link, rank) in ranks.collect():
        print("%s has rank: %s." % (link, rank))

    spark.stop()
test.txt
A B
C A
A C
B C